# data_merge.py - ë°ì´í„° ë³‘í•© ë° ê´€ë¦¬ ëª¨ë“ˆ (UTC & ts_min ì•ˆì „, 30ë¶„ë´‰ LogManager í˜¸í™˜)
# - ê°€ê²©: PRICE_DATA_DIR/raw/prices_YYYYMMDD.csv (ë˜ëŠ” prices.csv) ëª¨ì•„ì„œ ì‚¬ìš©
# - ê±°ë˜: logs/trades/YYYYMMDD.csv (LogManagerê°€ ì“°ëŠ” ì¼ìë³„ í†µí•© íŒŒì¼)
# - í”¼ì²˜: logs/features/features_YYYYMMDD.csv
# - ts_min í†µì¼ ìƒì„± (ê°€ê²©: timestamp, ê±°ë˜: bar30_end, í”¼ì²˜: entry_ts ê¸°ë³¸)
# - ë ˆê±°ì‹œ trades.csvë„ ìë™ í˜¸í™˜

import pandas as pd
import numpy as np
import os
import glob
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, Tuple, List


class DataMerger:
    """ì‹¤ì‹œê°„ ë°ì´í„° ë³‘í•© ë° ê´€ë¦¬ (30ë¶„ë´‰ ë¡œê·¸ ìŠ¤í‚¤ë§ˆ í˜¸í™˜)"""

    def __init__(self, config):
        self.config = config
        self.merged_data = None

    # ========================
    # ê¸°ë³¸ ìœ í‹¸
    # ========================
    @staticmethod
    def _to_utc_series(s: pd.Series) -> pd.Series:
        """ë¬¸ìì—´/naive datetimeì„ UTC-aware Timestampë¡œ ë³€í™˜"""
        if s is None:
            return pd.Series([], dtype='datetime64[ns, UTC]')
        return pd.to_datetime(s, errors='coerce', utc=True)

    @staticmethod
    def _dedup_columns(df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty:
            return df
        return df.loc[:, ~df.columns.duplicated()].copy()

    @classmethod
    def _ensure_ts_min(cls, df: pd.DataFrame, time_col: str) -> pd.DataFrame:
        """
        df[time_col]ì„ UTCë¡œ ë³€í™˜í•˜ê³  ë¶„ë‹¨ìœ„ë¡œ ë‚´ë¦¼í•œ 'ts_min' ì»¬ëŸ¼ ìƒì„±(í•­ìƒ ì¬ê³„ì‚°)
        """
        df = df.copy()
        if 'ts_min' in df.columns:
            df.drop(columns=['ts_min'], inplace=True)
        if time_col not in df.columns:
            df['ts_min'] = pd.NaT
            return df
        ts = cls._to_utc_series(df[time_col])
        df[time_col] = ts
        df['ts_min'] = ts.dt.floor('T')
        return df

    # ========================
    # ê°€ê²© ë¡œë”©
    # ========================
    def _price_files(self) -> List[str]:
        raw_dir = os.path.join(self.config.PRICE_DATA_DIR, 'raw')
        files = sorted(glob.glob(os.path.join(raw_dir, 'prices_*.csv')))
        # ë°±ì—… í”Œëœ: ë‹¨ì¼ prices.csvê°€ ìˆì„ ìˆ˜ë„ ìˆìŒ
        alt = os.path.join(self.config.PRICE_DATA_DIR, 'prices.csv')
        if os.path.exists(alt):
            files.append(alt)
        return files

    def load_price_data(self,
                        start_date: Optional[datetime] = None,
                        end_date: Optional[datetime] = None) -> pd.DataFrame:
        files = self._price_files()
        if not files:
            return pd.DataFrame()

        dfs = []
        for fp in files:
            try:
                df = pd.read_csv(fp)
            except Exception:
                continue
            df = self._dedup_columns(df)
            # í•­ìƒ ts_min ì¬ê³„ì‚°
            if 'ts_min' in df.columns:
                df.drop(columns=['ts_min'], inplace=True, errors='ignore')
            if 'timestamp' in df.columns:
                df['timestamp'] = self._to_utc_series(df['timestamp'])
                if start_date is not None:
                    s = pd.to_datetime(start_date, utc=True)
                    df = df[df['timestamp'] >= s]
                if end_date is not None:
                    e = pd.to_datetime(end_date, utc=True)
                    df = df[df['timestamp'] <= e]
            dfs.append(df)

        if not dfs:
            return pd.DataFrame()

        price = pd.concat(dfs, ignore_index=True)
        price = self._dedup_columns(price)
        if 'timestamp' in price.columns:
            price = price.sort_values('timestamp').drop_duplicates(subset=['timestamp'], keep='last')
        return price

    # ========================
    # ê±°ë˜ ë¡œë”© (LogManager í¬ë§· ìš°ì„ )
    # ========================
    def _trade_files_by_days(self, days: int = 7) -> List[str]:
        """
        logs/trades/YYYYMMDD.csv ìµœê·¼ Nì¼ íŒŒì¼
        """
        out = []
        base = self.config.TRADE_LOG_DIR
        now = datetime.now(timezone.utc)
        for i in range(days):
            d = (now - timedelta(days=i)).strftime("%Y%m%d")
            fp = base / f"{d}.csv"
            if fp.exists():
                out.append(str(fp))
        return sorted(out)

    def load_trade_logs(self, days: int = 7, include_open: bool = False, join_meta: bool = True) -> pd.DataFrame:
        """
        LogManagerê°€ ìƒì„±í•œ ë‚ ì§œë³„ íŠ¸ë ˆì´ë“œ ë¡œê·¸ë¥¼ ë¡œë“œí•´ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ì–´ëŒ‘íŠ¸.
        (ê¸°ì¡´ load_trade_logs ëŒ€ì²´ìš©)
        """
        from datetime import datetime, timezone, timedelta
        import glob
        base_dir = self.config.TRADE_LOG_DIR
        dfs = []

        for i in range(days):
            d = (datetime.now(timezone.utc) - timedelta(days=i)).strftime("%Y%m%d")
            try:
                daily_path = self.config.get_log_path('trade', d)
            except Exception:
                daily_path = Path(base_dir) / f"{d}.csv"

            if os.path.exists(daily_path):
                df = pd.read_csv(daily_path)
                if df.empty:
                    continue

                # ì‹œê°„/ìˆ«ì ë³€í™˜
                for col in ['entry_ts', 'label_ts', 'bar30_start', 'bar30_end', 'cross_time']:
                    if col in df.columns:
                        df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')

                for col in ['result','entry_price','label_price','p_at_entry','dp_at_entry','regime']:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                df['entry_time'] = df.get('entry_ts', pd.NaT)
                df['exit_time']  = df.get('label_ts', pd.NaT)
                df['p_up']       = df.get('p_at_entry', np.nan)

                if 'side' in df.columns:
                    side = df['side'].astype(str).str.upper()
                    df['direction'] = np.where(side == 'LONG', 1, np.where(side == 'SHORT', 0, np.nan))
                elif 'direction' in df.columns:
                    df['direction'] = pd.to_numeric(df['direction'], errors='coerce')
                else:
                    df['direction'] = np.nan

                if not include_open:
                    if 'status' in df.columns:
                        df = df[df['status'] == 'CLOSED']
                    else:
                        df = df[df['result'].notna()]

                df['regime'] = df.get('regime', 0).fillna(0)

                keep = [
                    'trade_id','entry_time','exit_time','direction','p_up','result',
                    'entry_price','label_price','regime','side','p_at_entry','dp_at_entry',
                    'bar30_start','bar30_end','status','model_ver','feature_ver','filter_ver','cutoff_ver'
                ]
                df = df[[c for c in keep if c in df.columns]]

                # ë©”íƒ€ ë³‘í•©
                if join_meta:
                    meta_path = Path(base_dir) / 'meta' / f"{d}_meta.jsonl"
                    if meta_path.exists():
                        records = []
                        with open(meta_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                try:
                                    records.append(json.loads(line.strip()))
                                except:
                                    pass
                        if records:
                            m = pd.DataFrame(records)
                            if 'timestamp' in m.columns:
                                m['timestamp'] = pd.to_datetime(m['timestamp'], utc=True, errors='coerce')
                            if 'trade_id' in m.columns:
                                meta_cols = [c for c in ['trade_id','stake_recommended','model_version'] if c in m.columns]
                                m = m[meta_cols].drop_duplicates('trade_id', keep='last')
                                df = df.merge(m, on='trade_id', how='left')

                dfs.append(df)

        if not dfs:
            return pd.DataFrame()

        out = pd.concat(dfs, ignore_index=True)
        for col in ['entry_time','exit_time','bar30_start','bar30_end']:
            if col in out.columns:
                out[col] = pd.to_datetime(out[col], utc=True, errors='coerce')
        if 'entry_time' in out.columns:
            out = out.sort_values('entry_time')
        if 'p_up' not in out.columns and 'p_at_entry' in out.columns:
            out['p_up'] = out['p_at_entry']

        return out.reset_index(drop=True)


    # ========================
    # í”¼ì²˜ ë¡œë”© (LogManager í¬ë§·)
    # ========================
    def _feature_files_by_days(self, days: int = 7) -> List[str]:
        """
        logs/features/features_YYYYMMDD.csv ìµœê·¼ Nì¼ íŒŒì¼
        """
        out = []
        now = datetime.now(timezone.utc)
        for i in range(days):
            d = (now - timedelta(days=i)).strftime("%Y%m%d")
            fp = self.config.get_log_path('feature', d)
            if Path(fp).exists():
                out.append(str(fp))
        return sorted(out)

    def load_feature_logs(self, days: int = 7) -> pd.DataFrame:
        files = self._feature_files_by_days(days=days)
        if not files:
            return pd.DataFrame()

        dfs = []
        for fp in files:
            try:
                df = pd.read_csv(fp)
            except Exception:
                continue
            dfs.append(df)
        if not dfs:
            return pd.DataFrame()

        feats = pd.concat(dfs, ignore_index=True)
        feats = self._dedup_columns(feats)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìºìŠ¤íŒ…
        for c in ['bar30_start', 'bar30_end', 'pred_ts', 'entry_ts', 'label_ts']:
            if c in feats.columns:
                feats[c] = self._to_utc_series(feats[c])

        # í”¼ì²˜ ì •ë ¬/ì¤‘ë³µì œê±°
        # ts_min = entry_ts(ìˆìœ¼ë©´) â†’ ì—†ìœ¼ë©´ bar30_end
        if 'entry_ts' in feats.columns and feats['entry_ts'].notna().any():
            feats = self._ensure_ts_min(feats, 'entry_ts')
        elif 'bar30_end' in feats.columns:
            feats = self._ensure_ts_min(feats, 'bar30_end')
        else:
            feats['ts_min'] = pd.NaT

        feats = feats.sort_values('ts_min').drop_duplicates(subset=['ts_min'], keep='last')
        return feats

    # ========================
    # ë³‘í•©
    # ========================
    def merge_all_data(self, price_days: int = 7, trade_days: int = 7, feature_days: int = 7) -> pd.DataFrame:
        """
        ê°€ê²©/ê±°ë˜/í”¼ì²˜ë¥¼ ts_min ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        - ê°€ê²©: ì •í™• ë§¤ì¹­
        - ê±°ë˜: backward asof(ìµœëŒ€ 5ë¶„ í—ˆìš©)
        - í”¼ì²˜: ì •í™• ë§¤ì¹­
        """
        print("ë°ì´í„° ë³‘í•© ì‹œì‘...")

        price = self.load_price_data()  # ê°€ê²©ì€ ì „ì²´ íŒŒì¼ì—ì„œ ìë™ í•„í„°
        trades = self.load_trade_logs(days=trade_days)
        feats = self.load_feature_logs(days=feature_days)

        frames = []
        if not price.empty and 'timestamp' in price.columns:
            price = self._ensure_ts_min(price, 'timestamp')
            frames.append(price[['ts_min']].dropna())
        if not trades.empty:
            frames.append(trades[['ts_min']].dropna())
        if not feats.empty:
            frames.append(feats[['ts_min']].dropna())

        if not frames:
            print("ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        base = pd.concat(frames, ignore_index=True).drop_duplicates().sort_values('ts_min')
        merged = base.copy()

        # ê°€ê²©: ì •í™• ì¡°ì¸
        if not price.empty:
            right_p = price.drop(columns=['timestamp'], errors='ignore').drop_duplicates('ts_min', keep='last')
            merged = merged.merge(right_p, on='ts_min', how='left')

        # ê±°ë˜: ê°€ì¥ ê°€ê¹Œìš´ ì´ì „ ì‹œì  asof (5ë¶„ í—ˆìš©)
        if not trades.empty:
            right_t = trades.drop_duplicates('ts_min', keep='last')
            merged = pd.merge_asof(
                merged.sort_values('ts_min'),
                right_t.sort_values('ts_min'),
                on='ts_min',
                direction='backward',
                tolerance=pd.Timedelta('5min'),
                suffixes=('', '_trade')
            )

        # í”¼ì²˜: ì •í™• ì¡°ì¸
        if not feats.empty:
            right_f = feats.drop_duplicates('ts_min', keep='last')
            merged = merged.merge(right_f, on='ts_min', how='left', suffixes=('', '_feature'))

        # ëŒ€í‘œ timestamp
        if 'timestamp' in merged.columns and pd.api.types.is_datetime64_any_dtype(merged['timestamp']):
            ts = merged['timestamp']
        else:
            ts = merged['ts_min']
        merged['timestamp'] = ts

        merged = self._dedup_columns(merged).sort_values('ts_min').reset_index(drop=True)
        self.merged_data = merged

        # ë¦¬í¬íŠ¸
        print("\n" + "="*60)
        print("ë³‘í•© ì™„ë£Œ:")
        print("="*60)
        print(f"- ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(merged):,}")
        print(f"- ì‹œì‘ ì‹œê°„: {merged['timestamp'].min()}")
        print(f"- ì¢…ë£Œ ì‹œê°„: {merged['timestamp'].max()}")

        if 'trade_id' in merged.columns:
            tc = merged['trade_id'].notna().sum()
            print(f"- ê±°ë˜ ê¸°ë¡ ìˆ˜: {tc:,}")
            trades_with_price = merged[merged['trade_id'].notna() & merged.get('close').notna()]
            print(f"- ê°€ê²© ë§¤ì¹­ëœ ê±°ë˜: {len(trades_with_price):,}ê±´")
            missing = tc - len(trades_with_price)
            if missing > 0:
                print(f"  âš ï¸ ê°€ê²© ëˆ„ë½: {missing}ê±´ (í•™ìŠµ ì œì™¸ë¨)")

            if 'result' in merged.columns:
                wr = merged['result'].dropna()
                if not wr.empty:
                    wins = (wr == 1).sum()
                    total = len(wr)
                    print(f"- ìŠ¹ë¥ : {wr.mean()*100:.2f}% ({wins}/{total})")

            if 'regime' in merged.columns:
                print("\n[ë ˆì§ ë¶„í¬]")
                regime_data = merged[merged['trade_id'].notna()]['regime']
                labels = {1: "UP ğŸŸ¢", -1: "DOWN ğŸ”´", 0: "FLAT âšª"}
                total_with_regime = regime_data.notna().sum()
                if total_with_regime > 0:
                    for rv, cnt in regime_data.value_counts().sort_index().items():
                        name = labels.get(int(rv), f"REGIME-{int(rv)}")
                        pct = (cnt / total_with_regime) * 100
                        print(f"  {name:10s}: {cnt:4d}ê±´ ({pct:5.1f}%)")
        print("="*60 + "\n")

        return merged

    # ========================
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    # ========================
    def build_balanced_training(self, df: pd.DataFrame, min_per_class: int = 2000, recent_days: int = 30) -> pd.DataFrame:
        df = df.dropna(subset=['target', 'timestamp']).copy()
        df['timestamp'] = self._to_utc_series(df['timestamp'])
        if df['timestamp'].isna().all():
            return df.sample(frac=1.0, random_state=42).reset_index(drop=True)

        recent_cut = df['timestamp'].max() - pd.Timedelta(days=recent_days)
        recent = df[df['timestamp'] >= recent_cut]
        up = recent[recent['target'] == 1]
        dn = recent[recent['target'] == 0]

        if len(up) < min_per_class:
            need = min_per_class - len(up)
            pool = df[(df['target'] == 1) & (df['timestamp'] < recent_cut)]
            take = min(need, len(pool))
            if take > 0:
                up = pd.concat([up, pool.sample(take, replace=(len(pool) < need), random_state=42)])

        if len(dn) < min_per_class:
            need = min_per_class - len(dn)
            pool = df[(df['target'] == 0) & (df['timestamp'] < recent_cut)]
            take = min(need, len(pool))
            if take > 0:
                dn = pd.concat([dn, pool.sample(take, replace=(len(pool) < need), random_state=42)])

        balanced = pd.concat([up, dn]).sample(frac=1.0, random_state=42).reset_index(drop=True)
        return balanced

    @staticmethod
    def dedupe_by_hash(df: pd.DataFrame, feature_cols: List[str], round_n: int = 4) -> pd.DataFrame:
        if df.empty:
            return df
        f = df[feature_cols].round(round_n)
        keys = f.apply(lambda r: hash(tuple(r.values)), axis=1)
        return df.loc[~keys.duplicated()].copy()

    def save_merged_data(self, df: Optional[pd.DataFrame] = None) -> bool:
        if df is None:
            df = self.merged_data
        if df is None or df.empty:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        os.makedirs(self.config.RESULT_DIR, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = os.path.join(self.config.RESULT_DIR, f'merged_data_{ts}.pkl')
        df.to_pickle(path)
        latest = os.path.join(self.config.RESULT_DIR, 'training_data.pkl')
        df.to_pickle(latest)
        print(f"ë³‘í•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {path}")
        return True

    def get_training_data(self,
                          lookback_days: int = 30,
                          apply_balance: bool = True,
                          apply_dedupe: bool = True,
                          dedupe_round: int = 4,
                          min_per_class: int = 2000) -> Tuple[Optional[pd.DataFrame], Optional[pd.Series]]:
        """
        í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„
        - ìµœê·¼ Nì¼ í•„í„°
        - FeatureEngineerë¡œ feature/target ìƒì„±
        - (ì˜µì…˜) í´ë˜ìŠ¤ ë°¸ëŸ°ì‹± + ë””ë“€í”„
        - â˜… regime ì»¬ëŸ¼ ë³´ì¡´/í†µê³„
        """
        latest_path = os.path.join(self.config.RESULT_DIR, 'training_data.pkl')
        if os.path.exists(latest_path):
            df = pd.read_pickle(latest_path)
        else:
            df = self.merge_all_data()
            if df is None or df.empty:
                return None, None

        if 'timestamp' in df.columns:
            cutoff = datetime.now(timezone.utc) - timedelta(days=lookback_days)
            ts = self._to_utc_series(df['timestamp'])
            df = df.assign(timestamp=ts)
            df = df[df['timestamp'] >= cutoff].copy()

        # FeatureEngineer ìœ„ì¹˜ ë³„ë„ ëª¨ë“ˆì´ë¼ë©´ ì•„ë˜ ì„í¬íŠ¸ ë¼ì¸ í™•ì¸
        from feature_engineer import FeatureEngineer
        fe = FeatureEngineer()

        X = fe.create_feature_pool(df)
        y = fe.create_target(df, window=self.config.PREDICTION_WINDOW)

        valid = y.notna()
        X = X[valid].copy()
        y = y[valid].copy()
        if X.empty or y.empty:
            return None, None

        if 'timestamp' in df.columns:
            try:
                ts_series = df.loc[X.index, 'timestamp']
            except Exception:
                ts_series = df['timestamp'].iloc[-len(X):].reset_index(drop=True)
                X = X.reset_index(drop=True)
                y = y.reset_index(drop=True)
        else:
            ts_series = pd.Series([pd.NaT]*len(X), dtype='datetime64[ns, UTC]')

        tmp = X.copy()
        tmp['target'] = y.values
        tmp['timestamp'] = ts_series.values

        feat_cols = list(X.columns)
        if apply_balance:
            tmp = self.build_balanced_training(tmp, min_per_class=min_per_class, recent_days=30)
        if apply_dedupe and len(tmp) > 0:
            tmp = self.dedupe_by_hash(tmp, feat_cols, round_n=dedupe_round)

        X_final = tmp[feat_cols].copy()
        y_final = tmp['target'].copy()

        # ë ˆì§ í†µê³„
        if 'regime' in X_final.columns:
            have = int(X_final['regime'].notna().sum())
            none = int(X_final['regime'].isna().sum())
            print("\n[í•™ìŠµ ë°ì´í„° ë ˆì§ ì •ë³´]")
            print(f"  ë ˆì§ ì •ë³´ ìˆìŒ: {have:,}ê±´")
            print(f"  ë ˆì§ ì •ë³´ ì—†ìŒ: {none:,}ê±´")

        return X_final, y_final

    # ==============
    # ë ˆê±°ì‹œ ì§€ì› (ì„ íƒ)
    # ==============
    def add_new_price_data(self, new_data: pd.DataFrame) -> bool:
        """ì‹¤ì‹œê°„ ê°€ê²© ì¶”ê°€ (ë¶„ë‹¨ìœ„ ë””ë“€í”„, ìµœì‹ ê°’ ìš°ì„ ) â€” ê¸°ì¡´ íŒŒì´í”„ì™€ í˜¸í™˜ìš©"""
        today = datetime.now().strftime("%Y%m%d")
        fp = os.path.join(self.config.PRICE_DATA_DIR, 'raw', f'prices_{today}.csv')

        new = new_data.copy()
        new['timestamp'] = pd.to_datetime(new['timestamp'], errors='coerce', utc=True)
        new['ts_min'] = new['timestamp'].dt.floor('T')

        if os.path.exists(fp):
            existing = pd.read_csv(fp)
            if 'timestamp' in existing.columns:
                existing['timestamp'] = pd.to_datetime(existing['timestamp'], errors='coerce', utc=True)
                existing['ts_min'] = existing['timestamp'].dt.floor('T')
            merged = pd.concat([existing, new], ignore_index=True)
        else:
            merged = new

        merged = merged.sort_values('timestamp').drop_duplicates(subset=['ts_min'], keep='last')
        merged.drop(columns=['ts_min'], inplace=True, errors='ignore')
        merged.to_csv(fp, index=False, encoding='utf-8-sig')
        print(f"ê°€ê²© ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: {len(new_data)} ë ˆì½”ë“œ")
        return True

    def cleanup_old_data(self, days_to_keep: int = 90):
        """ì˜¤ë˜ëœ ê°€ê²© raw íŒŒì¼ ì •ë¦¬ & ë ˆê±°ì‹œ ê±°ë˜ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ(ì˜µì…˜)"""
        cutoff = datetime.now() - timedelta(days=days_to_keep)

        price_files = glob.glob(os.path.join(self.config.PRICE_DATA_DIR, 'raw', '*.csv'))
        for file in price_files:
            filename = os.path.basename(file)
            if filename.startswith('prices_'):
                date_str = filename.replace('prices_', '').replace('.csv', '')
                try:
                    file_date = datetime.strptime(date_str, "%Y%m%d")
                    if file_date < cutoff:
                        os.remove(file)
                        print(f"ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {filename}")
                except Exception:
                    continue


class DataValidator:
    """ë°ì´í„° ê²€ì¦"""

    @staticmethod
    def validate_price_data(df: pd.DataFrame):
        issues = []
        required = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing = [c for c in required if c not in df.columns]
        if missing:
            issues.append(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

        if all(c in df.columns for c in ['open','high','low','close']):
            invalid_high = df[df['high'] < df[['open','close']].max(axis=1)]
            if not invalid_high.empty:
                issues.append(f"ì˜ëª»ëœ ê³ ê°€: {len(invalid_high)} ë ˆì½”ë“œ")
            invalid_low = df[df['low'] > df[['open','close']].min(axis=1)]
            if not invalid_low.empty:
                issues.append(f"ì˜ëª»ëœ ì €ê°€: {len(invalid_low)} ë ˆì½”ë“œ")

        key = 'ts_min' if 'ts_min' in df.columns else 'timestamp'
        if key in df.columns:
            d0 = df.dropna(subset=[key])
            dups = d0[d0.duplicated(subset=[key], keep=False)]
            if not dups.empty:
                issues.append(f"ì¤‘ë³µ {key}: {len(dups)} ë ˆì½”ë“œ")

        if set(required).issubset(df.columns):
            nulls = df[required].isnull().sum()
            if nulls.any():
                issues.append(f"ê²°ì¸¡ì¹˜: {nulls[nulls > 0].to_dict()}")

        return len(issues) == 0, issues

    @staticmethod
    def validate_trade_logs(df: pd.DataFrame):
        """
        ê±°ë˜ ë¡œê·¸ ê²€ì¦ (30ë¶„ë´‰ LogManager ìŠ¤í‚¤ë§ˆ)
          - í•„ìˆ˜: trade_id, bar30_end(or entry_ts), side, result(ì„ íƒ)
          - regime âˆˆ {-1,0,1}
        """
        issues = []
        required_any_time = [('bar30_end', 'entry_ts')]  # ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
        required = ['trade_id', 'side']

        missing = [c for c in required if c not in df.columns]
        if missing:
            issues.append(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

        ok_time = ('bar30_end' in df.columns) or ('entry_ts' in df.columns)
        if not ok_time:
            issues.append("ëˆ„ë½ëœ ì‹œê°„ ì»¬ëŸ¼: bar30_end ë˜ëŠ” entry_ts í•„ìš”")

        if 'trade_id' in df.columns:
            dups = df[df.duplicated(subset=['trade_id'], keep=False)]
            if not dups.empty:
                issues.append(f"ì¤‘ë³µ trade_id: {len(dups)} ë ˆì½”ë“œ")

        if 'side' in df.columns:
            bad = df[~df['side'].isin(['LONG', 'SHORT'])]
            if not bad.empty:
                issues.append(f"ì˜ëª»ëœ side ê°’: {len(bad)} ë ˆì½”ë“œ")

        if 'regime' in df.columns:
            regime_data = pd.to_numeric(df['regime'], errors='coerce').dropna()
            bad_regime = regime_data[~regime_data.isin([-1, 0, 1])]
            if not bad_regime.empty:
                issues.append(f"ì˜ëª»ëœ regime ê°’: {len(bad_regime)} ë ˆì½”ë“œ")

        return len(issues) == 0, issues


# =========================
# ë‹¨ë… í…ŒìŠ¤íŠ¸
# =========================
if __name__ == "__main__":
    from config import Config

    dm = DataMerger(Config)

    merged = dm.merge_all_data()
    if not merged.empty:
        merged = merged.dropna(subset=['open','high','low','close','volume'], how='any')
        dm.save_merged_data(merged)

        X, y = dm.get_training_data(lookback_days=30)
        if X is not None:
            print("\ní•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
            print(f"- í”¼ì²˜ shape: {X.shape}")
            print(f"- íƒ€ê²Ÿ shape: {y.shape}")
            print(f"- í´ë˜ìŠ¤ ë¶„í¬: {y.value_counts().to_dict()}")
