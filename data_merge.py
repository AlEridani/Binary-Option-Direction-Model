# data_merge.py - ë°ì´í„° ë³‘í•© ë° ê´€ë¦¬ ëª¨ë“ˆ (UTC & ts_min ì•ˆì „, ê±°ë˜/í”¼ì²˜ë§Œ ìˆì–´ë„ ë³‘í•© ê°€ëŠ¥, regime ì§€ì›)

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime, timedelta, timezone
import glob
from pathlib import Path


class DataMerger:
    """ì‹¤ì‹œê°„ ë°ì´í„° ë³‘í•© ë° ê´€ë¦¬"""

    def __init__(self, config):
        self.config = config
        self.merged_data = None

    # ------------------------
    # ë‚´ë¶€ í—¬í¼: ì•ˆì „í•œ ì‹œê°„/ì»¬ëŸ¼ ì²˜ë¦¬
    # ------------------------
    @staticmethod
    def _to_utc_series(s):
        """ë¬¸ìì—´/naive datetimeì„ UTC-aware Timestampë¡œ ë³€í™˜"""
        if s is None:
            return pd.Series([], dtype='datetime64[ns, UTC]')
        return pd.to_datetime(s, errors='coerce', utc=True)

    @staticmethod
    def _dedup_columns(df):
        """ì¤‘ë³µ ì»¬ëŸ¼ëª…ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€"""
        if df is None or df.empty:
            return df
        return df.loc[:, ~df.columns.duplicated()].copy()

    @classmethod
    def _ensure_ts_min(cls, df, time_col):
        """
        df[time_col]ì„ UTCë¡œ ë³€í™˜í•˜ê³  ë¶„ë‹¨ìœ„ë¡œ ë‚´ë¦¼í•œ 'ts_min' ì»¬ëŸ¼ì„ 'í•œ ë²ˆë§Œ' ë§Œë“ ë‹¤.
        - ê¸°ì¡´ì— ts_minì´ ìˆìœ¼ë©´ ì œê±° í›„ ì¬ê³„ì‚° (ì¤‘ë³µ ë¼ë²¨ ë°©ì§€)
        - time_colì´ ì—†ê±°ë‚˜ ì „ë¶€ NaTë©´ ts_minì€ NaT
        """
        df = df.copy()
        if 'ts_min' in df.columns:
            df = df.drop(columns=['ts_min'])
        if time_col not in df.columns:
            df['ts_min'] = pd.NaT
            return df

        ts = cls._to_utc_series(df[time_col])
        df[time_col] = ts
        df['ts_min'] = ts.dt.floor('T')
        return df

    # ------------------------
    # ë°ì´í„° ë¡œë”
    # ------------------------
    def load_price_data(self, start_date=None, end_date=None):
        """
        ê°€ê²© ë°ì´í„° ë¡œë“œ (UTC ë³€í™˜, timestamp ì¤‘ë³µ ì œê±°/ì •ë ¬)
        - ì›ì²œì— ts_minì´ ìˆì–´ë„ ë¬´ì‹œ (ì¬ê³„ì‚°)
        """
        price_dir = os.path.join(self.config.PRICE_DATA_DIR, 'raw')
        files = glob.glob(os.path.join(price_dir, '*.csv'))

        if not files:
            return pd.DataFrame()

        dfs = []
        for file in sorted(files):
            df = pd.read_csv(file)
            df = self._dedup_columns(df)
            if 'ts_min' in df.columns:
                df = df.drop(columns=['ts_min'])

            if 'timestamp' in df.columns:
                df['timestamp'] = self._to_utc_series(df['timestamp'])

                if start_date:
                    start_ts = self._to_utc_series(pd.Series([start_date]))[0]
                    df = df[df['timestamp'] >= start_ts]
                if end_date:
                    end_ts = self._to_utc_series(pd.Series([end_date]))[0]
                    df = df[df['timestamp'] <= end_ts]

            dfs.append(df)

        if not dfs:
            return pd.DataFrame()

        merged_df = pd.concat(dfs, ignore_index=True)
        merged_df = self._dedup_columns(merged_df)

        if 'timestamp' in merged_df.columns:
            merged_df = merged_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')

        return merged_df

    def load_trade_logs(self):
        """
        ê±°ë˜ ë¡œê·¸ ë¡œë“œ (UTC ë³€í™˜, regime ì»¬ëŸ¼ ì²˜ë¦¬)
        - regime ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (ë ˆê±°ì‹œ ë°ì´í„° ëŒ€ì‘)
        - ts_minì€ ë³‘í•© ì‹œ í†µì¼ ìƒì„±
        """
        path = os.path.join(self.config.TRADE_LOG_DIR, 'trades.csv')
        if not os.path.exists(path):
            return pd.DataFrame()

        df = pd.read_csv(path)
        df = self._dedup_columns(df)

        # â˜… regime ì»¬ëŸ¼ ì²˜ë¦¬ (ì—†ìœ¼ë©´ ì¶”ê°€)
        if 'regime' not in df.columns:
            df['regime'] = pd.NA
            print("â„¹ï¸  ë ˆê±°ì‹œ ë°ì´í„°: regime ì»¬ëŸ¼ ì¶”ê°€ (NaN)")
        else:
            # regime ì»¬ëŸ¼ì´ ìˆì–´ë„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            df['regime'] = pd.to_numeric(df['regime'], errors='coerce')

        # â˜… p_up ì»¬ëŸ¼ë„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (ìˆìœ¼ë©´)
        if 'p_up' in df.columns:
            df['p_up'] = pd.to_numeric(df['p_up'], errors='coerce')

        if 'entry_time' in df.columns:
            df['entry_time'] = self._to_utc_series(df['entry_time'])
        if 'exit_time' in df.columns:
            df['exit_time'] = self._to_utc_series(df['exit_time'])

        if 'ts_min' in df.columns:
            df = df.drop(columns=['ts_min'])
        
        return df

    def load_feature_logs(self):
        """í”¼ì²˜ ë¡œê·¸ ë¡œë“œ (UTC ë³€í™˜), ts_minì€ ë³‘í•© ì‹œ í†µì¼ ìƒì„±"""
        feature_dir = self.config.FEATURE_LOG_DIR
        files = glob.glob(os.path.join(feature_dir, 'features_*.csv'))
        if not files:
            return pd.DataFrame()

        dfs = []
        for file in sorted(files):
            df = pd.read_csv(file)
            df = self._dedup_columns(df)
            if 'timestamp' in df.columns:
                df['timestamp'] = self._to_utc_series(df['timestamp'])
            if 'ts_min' in df.columns:
                df = df.drop(columns=['ts_min'])
            dfs.append(df)

        if not dfs:
            return pd.DataFrame()

        merged_df = pd.concat(dfs, ignore_index=True)
        merged_df = self._dedup_columns(merged_df)
        if 'timestamp' in merged_df.columns:
            merged_df = merged_df.drop_duplicates().sort_values('timestamp')
        return merged_df

    # ------------------------
    # ë³‘í•©
    # ------------------------
    def merge_all_data(self):
        """
        ëª¨ë“  ë°ì´í„° ë³‘í•© (ts_min ê¸°ì¤€)
        - ê°€ê²©/ê±°ë˜/í”¼ì²˜ ê°ê°ì—ì„œ ts_min ìƒì„±(1íšŒ) í›„ ë³‘í•©
        - ê°€ê²© ë°ì´í„°ê°€ ì—†ì–´ë„ ê±°ë˜/í”¼ì²˜ë§Œìœ¼ë¡œ ë³‘í•© ê°€ëŠ¥
        - ìµœì¢… timestampëŠ” (ê°€ëŠ¥í•˜ë©´) ê°€ê²©ì˜ timestamp, ì—†ìœ¼ë©´ ts_min
        - â˜… regime ì»¬ëŸ¼ í¬í•¨ í†µê³„ ì¶œë ¥
        """
        print("ë°ì´í„° ë³‘í•© ì‹œì‘...")

        price = self.load_price_data()
        trades = self.load_trade_logs()
        feats = self.load_feature_logs()

        frames = []
        if not price.empty and 'timestamp' in price.columns:
            price = self._ensure_ts_min(price, 'timestamp')
            frames.append(price[['ts_min']].dropna())
        if not trades.empty and 'entry_time' in trades.columns:
            trades = self._ensure_ts_min(trades, 'entry_time')
            frames.append(trades[['ts_min']].dropna())
        if not feats.empty and 'timestamp' in feats.columns:
            feats = self._ensure_ts_min(feats, 'timestamp')
            frames.append(feats[['ts_min']].dropna())

        if not frames:
            print("ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        base = pd.concat(frames).drop_duplicates().sort_values('ts_min')

        merged = base.copy()

        if not price.empty:
            right = price.drop(columns=['timestamp'], errors='ignore')
            right = right.drop_duplicates('ts_min', keep='last')
            merged = merged.merge(right, on='ts_min', how='left')

        if not trades.empty:
            keep_cols = [c for c in trades.columns if c not in ['entry_time', 'exit_time', 'ts_min']]
            right = trades[['ts_min'] + keep_cols].drop_duplicates('ts_min', keep='last')
            merged = merged.merge(right, on='ts_min', how='left', suffixes=('', '_trade'))

        if not feats.empty:
            right = feats.drop(columns=['timestamp', 'ts_min'], errors='ignore')
            right = pd.concat([feats[['ts_min']], right], axis=1).drop_duplicates('ts_min', keep='last')           
            merged = merged.merge(right, on='ts_min', how='left', suffixes=('', '_feature'))

        # ëŒ€í‘œ timestamp ìƒì„±
        if 'timestamp' in merged.columns and pd.api.types.is_datetime64_any_dtype(merged['timestamp']):
            ts = merged['timestamp']
        else:
            ts = merged['ts_min']
        merged['timestamp'] = ts

        merged = self._dedup_columns(merged).sort_values('ts_min')
        self.merged_data = merged

        # â˜… í†µê³„ ì¶œë ¥ (regime í¬í•¨)
        print("\n" + "="*60)
        print("ë³‘í•© ì™„ë£Œ:")
        print("="*60)
        print(f"- ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(merged):,}")
        print(f"- ì‹œì‘ ì‹œê°„: {merged['timestamp'].min()}")
        print(f"- ì¢…ë£Œ ì‹œê°„: {merged['timestamp'].max()}")

        if 'trade_id' in merged.columns:
            tc = merged['trade_id'].notna().sum()
            print(f"- ê±°ë˜ ê¸°ë¡ ìˆ˜: {tc:,}")
            
            if 'result' in merged.columns:
                wr = merged['result'].dropna()
                if not wr.empty:
                    wins = (wr == 1).sum()
                    total = len(wr)
                    print(f"- ìŠ¹ë¥ : {wr.mean()*100:.2f}% ({wins}/{total})")
            
            # â˜… ë ˆì§ ë¶„í¬ í†µê³„
            if 'regime' in merged.columns:
                print("\n[ë ˆì§ ë¶„í¬]")
                regime_data = merged[merged['trade_id'].notna()]['regime']
                
                regime_labels = {
                    0.0: "UP íŠ¸ë Œë“œ ğŸŸ¢",
                    1.0: "DOWN íŠ¸ë Œë“œ ğŸ”´",
                    2.0: "FLAT íš¡ë³´ âšª"
                }
                
                total_with_regime = regime_data.notna().sum()
                total_without_regime = regime_data.isna().sum()
                
                if total_with_regime > 0:
                    regime_counts = regime_data.value_counts().sort_index()
                    for regime_val, count in regime_counts.items():
                        regime_name = regime_labels.get(regime_val, f"REGIME-{int(regime_val)}")
                        pct = (count / total_with_regime) * 100
                        print(f"  {regime_name:20s}: {count:4d}ê±´ ({pct:5.1f}%)")
                
                if total_without_regime > 0:
                    print(f"  {'ë ˆê±°ì‹œ (N/A)':20s}: {total_without_regime:4d}ê±´")
                
                # â˜… ë ˆì§ë³„ ìŠ¹ë¥  (resultê°€ ìˆìœ¼ë©´)
                if 'result' in merged.columns:
                    print("\n[ë ˆì§ë³„ ìŠ¹ë¥ ]")
                    regime_result = merged[merged['regime'].notna() & merged['result'].notna()]
                    
                    if len(regime_result) > 0:
                        for regime_val in sorted(regime_result['regime'].unique()):
                            regime_mask = regime_result['regime'] == regime_val
                            regime_subset = regime_result[regime_mask]
                            
                            wins = (regime_subset['result'] == 1).sum()
                            total = len(regime_subset)
                            win_rate = wins / total if total > 0 else 0
                            
                            regime_name = regime_labels.get(regime_val, f"REGIME-{int(regime_val)}")
                            
                            # ìŠ¹ë¥ ì— ë”°ë¥¸ ì´ëª¨ì§€
                            if win_rate >= 0.60:
                                emoji = "ğŸ”¥"
                            elif win_rate >= 0.55:
                                emoji = "âœ…"
                            elif win_rate >= 0.50:
                                emoji = "âš ï¸"
                            else:
                                emoji = "âŒ"
                            
                            print(f"  {regime_name:20s}: {win_rate*100:5.1f}% ({wins:3d}/{total:3d}) {emoji}")

        print("="*60 + "\n")

        return merged

    # ------------------------
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    # ------------------------
    def build_balanced_training(self, df, min_per_class=2000, recent_days=30):
        """
        ìµœê·¼ êµ¬ê°„ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ë˜, ë¶€ì¡± í´ë˜ìŠ¤ëŠ” ê³¼ê±°ì—ì„œ ë³´ì¶©í•´
        ìµœì†Œ í‘œë³¸ìˆ˜ë¥¼ ë§ì¶”ëŠ” ê· í˜• ë°ì´í„°ì…‹ êµ¬ì„±.
        df: feature + target + timestamp í¬í•¨ DataFrame
        """
        df = df.dropna(subset=['target', 'timestamp']).copy()
        df['timestamp'] = self._to_utc_series(df['timestamp'])
        if df['timestamp'].isna().all():
            return df.sample(frac=1.0, random_state=42).reset_index(drop=True)

        recent_cut = df['timestamp'].max() - pd.Timedelta(days=recent_days)
        recent = df[df['timestamp'] >= recent_cut]
        up = recent[recent['target'] == 1]
        dn = recent[recent['target'] == 0]

        if len(up) < min_per_class:
            need = min_per_class - len(up)
            pool = df[(df['target'] == 1) & (df['timestamp'] < recent_cut)]
            take = min(need, len(pool))
            if take > 0:
                up = pd.concat([up, pool.sample(take, replace=(len(pool) < need), random_state=42)])

        if len(dn) < min_per_class:
            need = min_per_class - len(dn)
            pool = df[(df['target'] == 0) & (df['timestamp'] < recent_cut)]
            take = min(need, len(pool))
            if take > 0:
                dn = pd.concat([dn, pool.sample(take, replace=(len(pool) < need), random_state=42)])

        balanced = pd.concat([up, dn]).sample(frac=1.0, random_state=42).reset_index(drop=True)
        return balanced

    @staticmethod
    def dedupe_by_hash(df, feature_cols, round_n=4):
        """
        í”¼ì²˜ê°’ì„ ë°˜ì˜¬ë¦¼í•´ í•´ì‹œ í‚¤ë¡œ ìœ ì‚¬ ìƒ˜í”Œ ì œê±°.
        ì—°ì†ì§„ì… ë“±ìœ¼ë¡œ ë¹„ìŠ·í•œ ìƒ˜í”Œì´ ë„ë°°ë  ë•Œ ê³¼í•™ìŠµ/í¸í–¥ ì™„í™”.
        """
        if df.empty:
            return df
        f = df[feature_cols].round(round_n)
        keys = f.apply(lambda r: hash(tuple(r.values)), axis=1)
        return df.loc[~keys.duplicated()].copy()

    def save_merged_data(self, df=None):
        """ë³‘í•©ëœ ë°ì´í„° ì €ì¥ (í”¼í´ 2ê°œ: íƒ€ì„ìŠ¤íƒ¬í”„/ìµœì‹ )"""
        if df is None:
            df = self.merged_data
        if df is None or df.empty:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        os.makedirs(self.config.RESULT_DIR, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = os.path.join(self.config.RESULT_DIR, f'merged_data_{ts}.pkl')
        df.to_pickle(path)
        latest = os.path.join(self.config.RESULT_DIR, 'training_data.pkl')
        df.to_pickle(latest)
        print(f"ë³‘í•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {path}")
        return True

    def get_training_data(self, lookback_days=30, apply_balance=True, apply_dedupe=True,
                          dedupe_round=4, min_per_class=2000):
        """
        í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„
        - ìµœì‹  ë³‘í•© ë°ì´í„° ì—†ìœ¼ë©´ ìƒˆë¡œ ë³‘í•©
        - ìµœê·¼ Nì¼ í•„í„°
        - FeatureEngineerë¡œ feature/target ìƒì„±
        - (ì˜µì…˜) í´ë˜ìŠ¤ ë°¸ëŸ°ì‹± + ë””ë“€í”„ ì ìš©
        - â˜… regime ì»¬ëŸ¼ ë³´ì¡´
        """
        latest_path = os.path.join(self.config.RESULT_DIR, 'training_data.pkl')
        if os.path.exists(latest_path):
            df = pd.read_pickle(latest_path)
        else:
            df = self.merge_all_data()
            if df is None or df.empty:
                return None, None

        if 'timestamp' in df.columns:
            cutoff = datetime.now(timezone.utc) - timedelta(days=lookback_days)
            ts = self._to_utc_series(df['timestamp'])
            df = df.assign(timestamp=ts)
            df = df[df['timestamp'] >= cutoff].copy()

        from model_train import FeatureEngineer
        fe = FeatureEngineer()
        X = fe.create_feature_pool(df)
        y = fe.create_target(df, window=self.config.PREDICTION_WINDOW)

        valid = y.notna()
        X = X[valid].copy()
        y = y[valid].copy()
        if X.empty or y.empty:
            return None, None

        if 'timestamp' in df.columns:
            try:
                ts_series = df.loc[X.index, 'timestamp']
            except Exception:
                ts_series = df['timestamp'].iloc[-len(X):].reset_index(drop=True)
                X = X.reset_index(drop=True)
                y = y.reset_index(drop=True)
        else:
            ts_series = pd.Series([pd.NaT]*len(X), dtype='datetime64[ns, UTC]')

        tmp = X.copy()
        tmp['target'] = y.values
        tmp['timestamp'] = ts_series.values

        feat_cols = list(X.columns)
        if apply_balance:
            tmp = self.build_balanced_training(tmp, min_per_class=min_per_class, recent_days=30)
        if apply_dedupe and len(tmp) > 0:
            tmp = self.dedupe_by_hash(tmp, feat_cols, round_n=dedupe_round)

        X_final = tmp[feat_cols].copy()
        y_final = tmp['target'].copy()
        
        # â˜… regime ì»¬ëŸ¼ í†µê³„ ì¶œë ¥
        if 'regime' in X_final.columns:
            regime_with_data = X_final['regime'].notna().sum()
            regime_without_data = X_final['regime'].isna().sum()
            print(f"\n[í•™ìŠµ ë°ì´í„° ë ˆì§ ì •ë³´]")
            print(f"  ë ˆì§ ì •ë³´ ìˆìŒ: {regime_with_data:,}ê±´")
            print(f"  ë ˆì§ ì •ë³´ ì—†ìŒ: {regime_without_data:,}ê±´ (ë ˆê±°ì‹œ)")
        
        return X_final, y_final

    def update_trade_result(self, trade_id, result, profit_loss):
        """ê±°ë˜ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
        path = os.path.join(self.config.TRADE_LOG_DIR, 'trades.csv')
        if not os.path.exists(path):
            return False

        df = pd.read_csv(path)
        df = self._dedup_columns(df)
        if 'trade_id' not in df.columns:
            return False

        mask = df['trade_id'] == trade_id
        if mask.any():
            df.loc[mask, 'result'] = result
            df.loc[mask, 'profit_loss'] = profit_loss
            df.loc[mask, 'exit_time'] = datetime.now(timezone.utc).isoformat()
            df.to_csv(path, index=False, encoding='utf-8-sig')
            print(f"ê±°ë˜ {trade_id} ê²°ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
        return False

    def add_new_price_data(self, new_data):
        """ì‹¤ì‹œê°„ ì €ì¥ (ë¶„ë‹¨ìœ„ ë””ë“€í”„, ìµœì‹ ê°’ ìš°ì„ )"""
        today = datetime.now().strftime("%Y%m%d")
        fp = os.path.join(self.config.PRICE_DATA_DIR, 'raw', f'prices_{today}.csv')

        new = new_data.copy()
        new['timestamp'] = pd.to_datetime(new['timestamp'], errors='coerce', utc=True)
        new['ts_min'] = new['timestamp'].dt.floor('T')

        if os.path.exists(fp):
            existing = pd.read_csv(fp)
            if 'timestamp' in existing.columns:
                existing['timestamp'] = pd.to_datetime(existing['timestamp'], errors='coerce', utc=True)
                existing['ts_min'] = existing['timestamp'].dt.floor('T')
            merged = pd.concat([existing, new], ignore_index=True)
        else:
            merged = new

        merged = merged.sort_values('timestamp').drop_duplicates(subset=['ts_min'], keep='last')
        merged = merged.drop(columns=['ts_min'], errors='ignore')
        merged.to_csv(fp, index=False, encoding='utf-8-sig')
        print(f"ê°€ê²© ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: {len(new_data)} ë ˆì½”ë“œ")
        return True

    def cleanup_old_data(self, days_to_keep=90):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ & ê±°ë˜ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ"""
        cutoff = datetime.now() - timedelta(days=days_to_keep)

        price_files = glob.glob(os.path.join(self.config.PRICE_DATA_DIR, 'raw', '*.csv'))
        for file in price_files:
            filename = os.path.basename(file)
            if filename.startswith('prices_'):
                date_str = filename.replace('prices_', '').replace('.csv', '')
                try:
                    file_date = datetime.strptime(date_str, "%Y%m%d")
                    if file_date < cutoff:
                        os.remove(file)
                        print(f"ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {filename}")
                except Exception:
                    continue

        trade_log = os.path.join(self.config.TRADE_LOG_DIR, 'trades.csv')
        if os.path.exists(trade_log):
            df = pd.read_csv(trade_log)
            df = self._dedup_columns(df)
            if 'entry_time' in df.columns:
                df['entry_time'] = self._to_utc_series(df['entry_time'])
                cutoff_utc = cutoff.replace(tzinfo=timezone.utc)
                old = df[df['entry_time'] < cutoff_utc]
                if not old.empty:
                    archive = os.path.join(self.config.TRADE_LOG_DIR,
                                           f'trades_archive_{cutoff.strftime("%Y%m%d")}.csv')
                    old.to_csv(archive, index=False, encoding='utf-8-sig')
                    df = df[df['entry_time'] >= cutoff_utc]
                    df.to_csv(trade_log, index=False, encoding='utf-8-sig')
                    print(f"ê±°ë˜ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {len(old)} ë ˆì½”ë“œ")


class DataValidator:
    """ë°ì´í„° ê²€ì¦ í´ë˜ìŠ¤"""

    @staticmethod
    def validate_price_data(df):
        """ê°€ê²© ë°ì´í„° ê²€ì¦"""
        issues = []
        required = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing = [c for c in required if c not in df.columns]
        if missing:
            issues.append(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

        if all(c in df.columns for c in ['open','high','low','close']):
            invalid_high = df[df['high'] < df[['open','close']].max(axis=1)]
            if not invalid_high.empty:
                issues.append(f"ì˜ëª»ëœ ê³ ê°€: {len(invalid_high)} ë ˆì½”ë“œ")
            invalid_low = df[df['low'] > df[['open','close']].min(axis=1)]
            if not invalid_low.empty:
                issues.append(f"ì˜ëª»ëœ ì €ê°€: {len(invalid_low)} ë ˆì½”ë“œ")

        key = 'ts_min' if 'ts_min' in df.columns else 'timestamp'
        if key in df.columns:
            d0 = df.dropna(subset=[key])
            dups = d0[d0.duplicated(subset=[key], keep=False)]
            if not dups.empty:
                issues.append(f"ì¤‘ë³µ {key}: {len(dups)} ë ˆì½”ë“œ")

        nulls = df[required].isnull().sum()
        if nulls.any():
            issues.append(f"ê²°ì¸¡ì¹˜: {nulls[nulls > 0].to_dict()}")

        return len(issues) == 0, issues

    @staticmethod
    def validate_trade_logs(df):
        """ê±°ë˜ ë¡œê·¸ ê²€ì¦ (í˜„ì¬ ìŠ¤í‚¤ë§ˆ: direction ì‚¬ìš©, regime ì„ íƒì )"""
        issues = []
        required = ['trade_id', 'entry_time', 'direction']
        missing = [c for c in required if c not in df.columns]
        if missing:
            issues.append(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

        if 'trade_id' in df.columns:
            dups = df[df.duplicated(subset=['trade_id'], keep=False)]
            if not dups.empty:
                issues.append(f"ì¤‘ë³µ trade_id: {len(dups)} ë ˆì½”ë“œ")

        if 'direction' in df.columns:
            bad = df[~df['direction'].isin([0, 1])]
            if not bad.empty:
                issues.append(f"ì˜ëª»ëœ direction ê°’: {len(bad)} ë ˆì½”ë“œ")
        
        # â˜… regime ê²€ì¦ (ì„ íƒì )
        if 'regime' in df.columns:
            regime_data = df['regime'].dropna()
            if len(regime_data) > 0:
                bad_regime = regime_data[~regime_data.isin([0, 1, 2])]
                if not bad_regime.empty:
                    issues.append(f"ì˜ëª»ëœ regime ê°’: {len(bad_regime)} ë ˆì½”ë“œ")

        return len(issues) == 0, issues


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from config import Config

    merger = DataMerger(Config)

    merged = merger.merge_all_data()
    
    if merged is not None:
        merged = merged.dropna(subset=['open','high','low','close','volume'])
        merger.save_merged_data()

        X, y = merger.get_training_data(lookback_days=30)
        if X is not None:
            print("\ní•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
            print(f"- í”¼ì²˜ shape: {X.shape}")
            print(f"- íƒ€ê²Ÿ shape: {y.shape}")
            print(f"- í´ë˜ìŠ¤ ë¶„í¬: {y.value_counts().to_dict()}")